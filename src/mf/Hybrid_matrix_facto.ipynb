{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "##### IMPORT LIBRARIES \n",
    "\n",
    "# Main library \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm # Track progress \n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Similarity metric\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "\n",
    "# Recommender systems\n",
    "import surprise as sp\n",
    "from surprise import Reader, Dataset, SVD, evaluate, accuracy  \n",
    "from surprise.model_selection import cross_validate, train_test_split, GridSearchCV\n",
    "\n",
    "# To create deep learning models\n",
    "from keras.layers import Input, Embedding, Reshape, Dot, Concatenate, Dense, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "# To create sparse matrices\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.sparse import vstack # To stack sparse matrices\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# To light fm\n",
    "# from lightfm import LightFM\n",
    "# from lightfm.evaluation import precision_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DATASETS \n",
    "\n",
    "# Normal train and test set (filtered and split)\n",
    "df_train= pd.read_csv('../../data/generated/inter_train.csv')\n",
    "df_train = df_train.dropna()\n",
    "df_test= pd.read_csv('../../data/generated/inter_test.csv') \n",
    "df_test = df_test.dropna()\n",
    "\n",
    "\n",
    "# Read as matrix format of train and test set\n",
    "X_train = pd.read_csv('../../data/generated/inter_train.csv').pivot_table(\n",
    "                            index='u', columns='i', values='rating', dropna=False)\n",
    "X_test = pd.read_csv('../../data/generated/inter_test.csv').pivot_table(\n",
    "                            index='u', columns='i', values='rating', dropna=False)\n",
    "\n",
    "# Fill in missing values with -1 and add 1 to each rating (want NaN to 0)\n",
    "train = X_train.fillna(-1)\n",
    "train = train.add(1)\n",
    "train2 = train.T  # make us save a few seconds in similarity computation \n",
    "\n",
    "\n",
    "# Full data - no train/test split \n",
    "full_data = pd.read_csv('../../data/generated/full_data_filtered.csv')\n",
    "full_data_matrix = full_data.pivot_table(index='u', columns='i', values='rating', dropna=False)\n",
    "\n",
    "\n",
    "# Create a dataframe giving the recipe's name along with its id 'i'\n",
    "names = pd.read_csv('../../data/generated/names.csv')\n",
    "names.columns=['i','name']\n",
    "names = names.set_index('i') # set 'i' as index, useful later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid reco with names of recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11120, 4378)\n"
     ]
    }
   ],
   "source": [
    "# Work directly on names that we need to tokenise \n",
    "list_names = names.name.tolist()   # do it directly on full_data to have the right dimension \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(list_names)\n",
    "# print(vectorizer.get_feature_names())\n",
    "print(X.shape)\n",
    "\n",
    "# Add sparse compact vectors of 0 and 1 in a list. \n",
    "tok_names = []\n",
    "d = {'sparse_vectors':[]}\n",
    "for i in range(names.shape[0]):\n",
    "    tok_names.append(X[i]) \n",
    "    d['sparse_vectors'].append(X[i])\n",
    "\n",
    "# Reimport dataset to get a normal index, not i as index\n",
    "names = pd.read_csv('../../data/generated/names.csv')\n",
    "names.columns=['i','name']\n",
    "\n",
    "# Join this to vectors created above\n",
    "data = pd.DataFrame(d)\n",
    "data = data.join(names)\n",
    "df_train = df_train.merge(data, on='i')\n",
    "df_test = df_test.merge(data, on='i')\n",
    "full_data = full_data.merge(data, on='i')  \n",
    "\n",
    "# Store those vectors\n",
    "# tokenised_names = full_data['sparse_vectors']\n",
    "tokenised_names = df_train['sparse_vectors']\n",
    "tokenised_names_test = df_test['sparse_vectors']\n",
    "\n",
    "# Stack the sparse matrices\n",
    "tokenised_names = vstack(tokenised_names)\n",
    "tokenised_names_test =  vstack(tokenised_names_test)\n",
    "\n",
    "# Leave as array for neural network implementation ease\n",
    "#tokenised_name = tokenised_names.toarray() \n",
    "#tokenised_name_test =  tokenised_names_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user- & recipe-id mapping\n",
    "df_filter= pd.read_csv('../../data/generated/inter_train.csv')\n",
    "user_id_mapping = {id:i for i, id in enumerate(df_filter['u'].unique())}\n",
    "recipe_id_mapping = {id:i for i, id in enumerate(df_filter['i'].unique())}\n",
    "\n",
    "# Create correctly mapped train & testset\n",
    "train_user_data = df_train['u'].map(user_id_mapping)\n",
    "train_recipe_data = df_train['i'].map(recipe_id_mapping)\n",
    "test_user_data = df_test['u'].map(user_id_mapping)\n",
    "test_recipe_data = df_test['i'].map(recipe_id_mapping)\n",
    "\n",
    "# Get input variable-sizes\n",
    "users = len(user_id_mapping)  #10007\n",
    "recipes = len(recipe_id_mapping) #11120\n",
    "embed_size_user = 10\n",
    "embed_size_recipe = 10\n",
    "\n",
    "\n",
    "# Set input layers. We input respectively 10007 and 11120 items of size 1 (u and i) \n",
    "user_id_input = Input(shape=[1], name='user')  # shape (None,1)\n",
    "recipe_id_input = Input(shape=[1], name='recipe') # shape (None,1)\n",
    "name_input = Input(shape=[4378], name='tokenised_names') # shape (None, 4378) #sparse = True\n",
    "\n",
    "\n",
    "# Create embedding layers for users and recipes\n",
    "user_embedding = Embedding(output_dim=embed_size_user, \n",
    "                           input_dim=users,\n",
    "                           input_length=1, \n",
    "                           name='user_embedding')(user_id_input)  # shape (None, 1, embedding_size)\n",
    "recipe_embedding = Embedding(output_dim=embed_size_recipe, \n",
    "                            input_dim=recipes,\n",
    "                            input_length=1, \n",
    "                            name='item_embedding')(recipe_id_input) # shape (None, 1, embedding_size)\n",
    "\n",
    "\n",
    "# Dimensionality reduction with Dense layers\n",
    "name_vectors = Dense(128, activation='relu')(name_input)\n",
    "name_vectors = Dense(32, activation='relu')(name_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandreduval/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 225338 samples, validate on 55925 samples\n",
      "Epoch 1/5\n",
      "225338/225338 [==============================] - 39s 172us/step - loss: 2.0635 - val_loss: 1.0406\n",
      "Epoch 2/5\n",
      "225338/225338 [==============================] - 42s 186us/step - loss: 0.6945 - val_loss: 1.0317\n",
      "Epoch 3/5\n",
      "225338/225338 [==============================] - 38s 170us/step - loss: 0.6652 - val_loss: 1.0283\n",
      "Epoch 4/5\n",
      "225338/225338 [==============================] - 36s 162us/step - loss: 0.6491 - val_loss: 1.0374\n",
      "Epoch 5/5\n",
      "225338/225338 [==============================] - 40s 177us/step - loss: 0.6346 - val_loss: 1.0380\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a44437160>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape the embedding layers (- like flattening)\n",
    "user_vector = Reshape([embed_size_user])(user_embedding) # shape (None, embedding_size)\n",
    "recipe_vector = Reshape([embed_size_recipe])(recipe_embedding) # shape (None, embedding_size)\n",
    "\n",
    "\n",
    "# Concatenate the reshaped embedding layers\n",
    "concat = Concatenate()([user_vector, recipe_vector, name_vectors])\n",
    "\n",
    "\n",
    "# Combine with dense layers\n",
    "dense = Dense(512, activation='relu')(concat)\n",
    "dense = Dropout(0.2)(dense)\n",
    "dense = Dense(128)(dense)\n",
    "y = Dense(1)(dense)\n",
    "\n",
    "\n",
    "# Instantiate a model given input and output layers. \n",
    "model3 = Model(inputs=[user_id_input, recipe_id_input, name_input], outputs=y)\n",
    "model3.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "\n",
    "# Fit model\n",
    "model3.fit([train_user_data, train_recipe_data, tokenised_names],\n",
    "          df_train['rating'],\n",
    "          batch_size= 1200, \n",
    "          epochs=5,\n",
    "           validation_data = ([test_user_data, test_recipe_data, tokenised_names_test], df_test['rating']),\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_rating</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>rating</th>\n",
       "      <th>u</th>\n",
       "      <th>i</th>\n",
       "      <th>sparse_vectors</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4.501110</td>\n",
       "      <td>174738</td>\n",
       "      <td>4.0</td>\n",
       "      <td>87</td>\n",
       "      <td>78551</td>\n",
       "      <td>(0, 3063)\\t0.4182780190955211\\n  (0, 990)\\t0...</td>\n",
       "      <td>crock pot potato chowder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8800</th>\n",
       "      <td>4.600294</td>\n",
       "      <td>292278</td>\n",
       "      <td>5.0</td>\n",
       "      <td>87</td>\n",
       "      <td>172148</td>\n",
       "      <td>(0, 2759)\\t0.7896216758775201\\n  (0, 3117)\\t...</td>\n",
       "      <td>oreo pudding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16572</th>\n",
       "      <td>4.566111</td>\n",
       "      <td>238734</td>\n",
       "      <td>5.0</td>\n",
       "      <td>87</td>\n",
       "      <td>142774</td>\n",
       "      <td>(0, 1156)\\t0.6097664458543058\\n  (0, 3548)\\t...</td>\n",
       "      <td>dirty shrimp in butter beer sauce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16597</th>\n",
       "      <td>5.015951</td>\n",
       "      <td>268995</td>\n",
       "      <td>5.0</td>\n",
       "      <td>87</td>\n",
       "      <td>103884</td>\n",
       "      <td>(0, 727)\\t0.71564958063136\\n  (0, 3894)\\t0.6...</td>\n",
       "      <td>taco cheesecake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16604</th>\n",
       "      <td>4.830988</td>\n",
       "      <td>275263</td>\n",
       "      <td>3.0</td>\n",
       "      <td>87</td>\n",
       "      <td>149422</td>\n",
       "      <td>(0, 591)\\t0.2883718187858596\\n  (0, 1598)\\t0...</td>\n",
       "      <td>gingerbread gingerbread cake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16608</th>\n",
       "      <td>4.747349</td>\n",
       "      <td>281036</td>\n",
       "      <td>5.0</td>\n",
       "      <td>87</td>\n",
       "      <td>114081</td>\n",
       "      <td>(0, 1486)\\t0.7381545937583465\\n  (0, 2991)\\t...</td>\n",
       "      <td>black forest pizza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16610</th>\n",
       "      <td>4.788479</td>\n",
       "      <td>281094</td>\n",
       "      <td>3.0</td>\n",
       "      <td>87</td>\n",
       "      <td>148228</td>\n",
       "      <td>(0, 3486)\\t0.7041787357709044\\n  (0, 635)\\t0...</td>\n",
       "      <td>best seller caramel corn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16623</th>\n",
       "      <td>5.066853</td>\n",
       "      <td>349418</td>\n",
       "      <td>5.0</td>\n",
       "      <td>87</td>\n",
       "      <td>112084</td>\n",
       "      <td>(0, 1016)\\t0.5719906303358219\\n  (0, 3307)\\t...</td>\n",
       "      <td>pie crust cinnamon rolls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16629</th>\n",
       "      <td>4.864528</td>\n",
       "      <td>367410</td>\n",
       "      <td>5.0</td>\n",
       "      <td>87</td>\n",
       "      <td>72975</td>\n",
       "      <td>(0, 2426)\\t0.6109239677447547\\n  (0, 2208)\\t...</td>\n",
       "      <td>lil  cheddar meatloaves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16643</th>\n",
       "      <td>4.801425</td>\n",
       "      <td>371050</td>\n",
       "      <td>5.0</td>\n",
       "      <td>87</td>\n",
       "      <td>115658</td>\n",
       "      <td>(0, 247)\\t0.609799794291513\\n  (0, 860)\\t0.4...</td>\n",
       "      <td>kittencal s bakery coconut cream pie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16663</th>\n",
       "      <td>4.406237</td>\n",
       "      <td>371069</td>\n",
       "      <td>3.0</td>\n",
       "      <td>87</td>\n",
       "      <td>99509</td>\n",
       "      <td>(0, 4233)\\t0.5643097057379017\\n  (0, 2567)\\t...</td>\n",
       "      <td>kittencal s no more watery meringue</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pred_rating  Unnamed: 0  rating   u       i  \\\n",
       "32        4.501110      174738     4.0  87   78551   \n",
       "8800      4.600294      292278     5.0  87  172148   \n",
       "16572     4.566111      238734     5.0  87  142774   \n",
       "16597     5.015951      268995     5.0  87  103884   \n",
       "16604     4.830988      275263     3.0  87  149422   \n",
       "16608     4.747349      281036     5.0  87  114081   \n",
       "16610     4.788479      281094     3.0  87  148228   \n",
       "16623     5.066853      349418     5.0  87  112084   \n",
       "16629     4.864528      367410     5.0  87   72975   \n",
       "16643     4.801425      371050     5.0  87  115658   \n",
       "16663     4.406237      371069     3.0  87   99509   \n",
       "\n",
       "                                          sparse_vectors  \\\n",
       "32       (0, 3063)\\t0.4182780190955211\\n  (0, 990)\\t0...   \n",
       "8800     (0, 2759)\\t0.7896216758775201\\n  (0, 3117)\\t...   \n",
       "16572    (0, 1156)\\t0.6097664458543058\\n  (0, 3548)\\t...   \n",
       "16597    (0, 727)\\t0.71564958063136\\n  (0, 3894)\\t0.6...   \n",
       "16604    (0, 591)\\t0.2883718187858596\\n  (0, 1598)\\t0...   \n",
       "16608    (0, 1486)\\t0.7381545937583465\\n  (0, 2991)\\t...   \n",
       "16610    (0, 3486)\\t0.7041787357709044\\n  (0, 635)\\t0...   \n",
       "16623    (0, 1016)\\t0.5719906303358219\\n  (0, 3307)\\t...   \n",
       "16629    (0, 2426)\\t0.6109239677447547\\n  (0, 2208)\\t...   \n",
       "16643    (0, 247)\\t0.609799794291513\\n  (0, 860)\\t0.4...   \n",
       "16663    (0, 4233)\\t0.5643097057379017\\n  (0, 2567)\\t...   \n",
       "\n",
       "                                       name  \n",
       "32                 crock pot potato chowder  \n",
       "8800                           oreo pudding  \n",
       "16572     dirty shrimp in butter beer sauce  \n",
       "16597                       taco cheesecake  \n",
       "16604          gingerbread gingerbread cake  \n",
       "16608                    black forest pizza  \n",
       "16610              best seller caramel corn  \n",
       "16623              pie crust cinnamon rolls  \n",
       "16629               lil  cheddar meatloaves  \n",
       "16643  kittencal s bakery coconut cream pie  \n",
       "16663   kittencal s no more watery meringue  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Investigate predictions on test set (unseen data)\n",
    "\n",
    "# Compute predictions and compare them to the true value\n",
    "y_pred = model3.predict([test_user_data, test_recipe_data, tokenised_names_test])\n",
    "\n",
    "# Add names\n",
    "# df_test = df_test.merge(names,on='i')\n",
    "y_pred = pd.DataFrame(y_pred)\n",
    "df_test_copy = df_test.reset_index(drop=True)\n",
    "y_pred = y_pred.join(df_test_copy)\n",
    "y_pred = y_pred.rename(columns={0:'pred_rating'})\n",
    "\n",
    "# Look at a user in particular\n",
    "y_pred[y_pred['u']==87]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>i</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6250</th>\n",
       "      <td>5.639419</td>\n",
       "      <td>98499</td>\n",
       "      <td>broiled lobster tails for 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4340</th>\n",
       "      <td>5.578915</td>\n",
       "      <td>67254</td>\n",
       "      <td>fried apples   stekte epler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4478</th>\n",
       "      <td>5.545162</td>\n",
       "      <td>69528</td>\n",
       "      <td>applewood farmhouse apple fritters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5847</th>\n",
       "      <td>5.533090</td>\n",
       "      <td>91978</td>\n",
       "      <td>julia child s cherry clafouti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2681</th>\n",
       "      <td>5.509263</td>\n",
       "      <td>40667</td>\n",
       "      <td>fruity grilled cheese sandwich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10350</th>\n",
       "      <td>5.506562</td>\n",
       "      <td>165260</td>\n",
       "      <td>pizza bagel bites  oamc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2246</th>\n",
       "      <td>5.502390</td>\n",
       "      <td>34399</td>\n",
       "      <td>spooktacular halloween graveyard cake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4477</th>\n",
       "      <td>5.501045</td>\n",
       "      <td>69483</td>\n",
       "      <td>not your ordinary chocolate chip cookies  liqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7199</th>\n",
       "      <td>5.498896</td>\n",
       "      <td>113704</td>\n",
       "      <td>nana s chocolate frosting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6569</th>\n",
       "      <td>5.495666</td>\n",
       "      <td>103571</td>\n",
       "      <td>my moms lemon bars are better than your moms l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          score       i                                               name\n",
       "6250   5.639419   98499                        broiled lobster tails for 2\n",
       "4340   5.578915   67254                        fried apples   stekte epler\n",
       "4478   5.545162   69528                 applewood farmhouse apple fritters\n",
       "5847   5.533090   91978                      julia child s cherry clafouti\n",
       "2681   5.509263   40667                     fruity grilled cheese sandwich\n",
       "10350  5.506562  165260                            pizza bagel bites  oamc\n",
       "2246   5.502390   34399              spooktacular halloween graveyard cake\n",
       "4477   5.501045   69483  not your ordinary chocolate chip cookies  liqu...\n",
       "7199   5.498896  113704                          nana s chocolate frosting\n",
       "6569   5.495666  103571  my moms lemon bars are better than your moms l..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Print best recommendation \n",
    "\n",
    "user_index = 87 \n",
    "# List of unrated recipes for a user - include train and test\n",
    "unrated_recipes = full_data_matrix.iloc[user_index][full_data_matrix.iloc[user_index].isna()].index.tolist()\n",
    "\n",
    "# Get tokenised names for each recipe, in the right order\n",
    "t_n = data['sparse_vectors']\n",
    "t_n = vstack(t_n)\n",
    "t_n =t_n.toarray()\n",
    "\n",
    "#len(lst)\n",
    "unrated_recipes = pd.Series(unrated_recipes)\n",
    "ur = unrated_recipes.map(recipe_id_mapping)\n",
    "lst = [87] * len(unrated_recipes)\n",
    "y_pred = model3.predict([lst, ur, t_n])\n",
    "\n",
    "y_pred = pd.DataFrame(y_pred)\n",
    "y_pred = y_pred.join(names)\n",
    "y_pred = y_pred.rename(columns ={0:'score'})\n",
    "y_pred.sort_values('score', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid with additional metadata in the middle layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with only tokenised names and recipe_id\n",
    "pp_recipes = pd.read_csv('../../data/pp_recipes.csv')\n",
    "raw_recipes = pd.read_csv('../../data/raw_recipes.csv')\n",
    "pp_recip = pp_recipes[['calorie_level','id']]\n",
    "raw_recip = raw_recipes[['minutes','n_steps','n_ingredients','id']]\n",
    "pp_recip = pp_recip.merge(raw_recip,on='id')\n",
    "# Normalise data pp_recip\n",
    "\n",
    "# Update my entire dataset \n",
    "full_data = full_data.rename(columns={\"recipe_id\": \"id\"})\n",
    "full_data = full_data.merge(pp_recip, on='id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 6 array(s), but instead got the following list of 3 arrays: [array([[    0],\n       [    1],\n       [    2],\n       ...,\n       [10005],\n       [ 9977],\n       [ 9982]]), array([[    0],\n       [    0],\n       [    0],\n       ...,\n       [11119],\n       [11119...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-bd5df7c140be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m            \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_user_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_recipe_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenised_names_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m           shuffle=True)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 6 array(s), but instead got the following list of 3 arrays: [array([[    0],\n       [    1],\n       [    2],\n       ...,\n       [10005],\n       [ 9977],\n       [ 9982]]), array([[    0],\n       [    0],\n       [    0],\n       ...,\n       [11119],\n       [11119..."
     ]
    }
   ],
   "source": [
    "# Create user- & recipe-id mapping\n",
    "user_id_mapping = {id:i for i, id in enumerate(full_data['u'].unique())}\n",
    "recipe_id_mapping = {id:i for i, id in enumerate(full_data['i'].unique())}\n",
    "\n",
    "# Create correctly mapped train & testset\n",
    "train_user_data = full_data['u'].map(user_id_mapping)\n",
    "train_recipe_data = full_data['i'].map(recipe_id_mapping)\n",
    "\n",
    "# Get input variable-sizes\n",
    "users = len(user_id_mapping)  #10007\n",
    "recipes = len(recipe_id_mapping) #11120\n",
    "embed_size_user = 10\n",
    "embed_size_recipe = 10\n",
    "\n",
    "\n",
    "# Set input layers. We input respectively 10007 and 11120 items of size 1 (u and i) \n",
    "user_id_input = Input(shape=[1], name='user')  # shape (None,1)\n",
    "recipe_id_input = Input(shape=[1], name='recipe') # shape (None,1)\n",
    "calorie_input = Input(shape=[1], name='calorie') # shape (None, 1) \n",
    "time_input =  Input(shape=[1], name='minutes')\n",
    "steps_input=  Input(shape=[1], name='steps')\n",
    "ing_input =  Input(shape=[1], name='ingredients')\n",
    "\n",
    "\n",
    "\n",
    "# Create embedding layers for users and recipes\n",
    "user_embedding = Embedding(output_dim=embed_size_user, \n",
    "                           input_dim=users,\n",
    "                           input_length=1, \n",
    "                           name='user_embedding')(user_id_input)  # shape (None, 1, embedding_size)\n",
    "recipe_embedding = Embedding(output_dim=embed_size_recipe, \n",
    "                            input_dim=recipes,\n",
    "                            input_length=1, \n",
    "                            name='item_embedding')(recipe_id_input) # shape (None, 1, embedding_size)\n",
    "\n",
    "# Augment it \n",
    "# name_vectors = Dense(16, activation='relu')(name_input)\n",
    "\n",
    "\n",
    "# Reshape the embedding layers (- like flattening)\n",
    "user_vector = Reshape([embed_size_user])(user_embedding) # shape (None, embedding_size)\n",
    "recipe_vector = Reshape([embed_size_recipe])(recipe_embedding) # shape (None, embedding_size)\n",
    "\n",
    "\n",
    "# Concatenate the reshaped embedding layers\n",
    "concat = Concatenate()([user_vector, recipe_vector, calorie_input, time_input, steps_input, ing_input])\n",
    "\n",
    "# Combine with dense layers\n",
    "dense = Dense(56)(concat)\n",
    "y = Dense(1)(dense)\n",
    "\n",
    "\n",
    "# Instantiate a model given input and output layers. \n",
    "model4 = Model(inputs=[user_id_input, recipe_id_input, calorie_input, time_input, steps_input, ing_input], outputs=y)\n",
    "model4.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "\n",
    "# Fit model\n",
    "model4.fit([train_user_data, train_recipe_data, tokenised_names],\n",
    "          df_train['rating'],\n",
    "          batch_size= 1200, \n",
    "          epochs=8,\n",
    "           validation_data = ([test_user_data, test_recipe_data, tokenised_names_test], df_test['rating']),\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Print best recommendation \n",
    "\n",
    "user_index = 87 \n",
    "# List of unrated recipes for a user - include train and test\n",
    "unrated_recipes = full_data_matrix.iloc[user_index][full_data_matrix.iloc[user_index].isna()].index.tolist()\n",
    "\n",
    "# Get tokenised names for each recipe, in the right order\n",
    "t_n = data['sparse_vectors']\n",
    "t_n = vstack(t_n)\n",
    "t_n =t_n.toarray()\n",
    "\n",
    "#len(lst)\n",
    "unrated_recipes = pd.Series(unrated_recipes)\n",
    "ur = unrated_recipes.map(recipe_id_mapping)\n",
    "lst = [87] * len(unrated_recipes)\n",
    "y_pred = model3.predict([lst, ur, t_n])\n",
    "\n",
    "y_pred = pd.DataFrame(y_pred)\n",
    "y_pred = y_pred.join(names)\n",
    "y_pred = y_pred.rename(columns ={0:'score'})\n",
    "y_pred.sort_values('score', ascending=False)[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
